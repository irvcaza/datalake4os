{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuaderno para cargar metadatos \n",
    "\n",
    "Este cuaderno toma un directorio de MinIO con estructura de datos abiertos y crea la definición para Hive-Metastore para cada una de las tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion de coneccion a MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Minio(\n",
    "        \"minio:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de ayuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea la definicion de la tabla en el directorio dado utilizando el diccionario de datos\n",
    "def create_hive_table(client, bucket, directory_object_name):\n",
    "    table_name = get_table_name(directory_object_name)\n",
    "    col_def = get_col_def(client, bucket, directory_object_name)\n",
    "    data_location = directory_object_name+\"conjunto_de_datos/\"\n",
    "    # TODO: revisar si la tabla ya existe \n",
    "    table_def = \"\"\"\n",
    "        CREATE EXTERNAL TABLE {} ({})\n",
    "        ROW FORMAT DELIMITED\n",
    "        FIELDS TERMINATED BY ','\n",
    "        LINES TERMINATED BY '\\\\n'\n",
    "        LOCATION 's3a://{}/{}'\n",
    "        TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "        \"\"\".format(table_name,col_def,bucket,data_location)\n",
    "    return table_def\n",
    "\n",
    "# Funcion para establecer el nombre de la tabla, en una version peeliminar es el nombre del directorio, pero se puede refinar para lograr algo mas conciso \n",
    "def get_table_name(directory_object_name):\n",
    "    return directory_object_name.split(\"/\")[-2]\n",
    "\n",
    "# Fucion que crea la definicion de las variables, nombre de la variable y tipo \n",
    "def get_col_def (client, bucket, directory_object_name):\n",
    "    data_dictionary = get_data_dictionary(client, bucket, directory_object_name)\n",
    "    # TODO: agregar la parte donde podemos cambiar los nombres \n",
    "    # TODO: revisar si este ese el roden real de las columans de la tabla \n",
    "    names = [x.upper() for x in data_dictionary[\"Variable\"]]\n",
    "    types = [get_type(x) for x in data_dictionary[\"Tipo\"]]\n",
    "    return \", \".join([\"{} {}\".format(x,y) for x,y in zip(names,types)])\n",
    "    \n",
    "# Obtiene el diccionario de datos del formato de datos abiertos para definir las varibles\n",
    "def get_data_dictionary(client, bucket, directory_object_name):\n",
    "    dic_location = [obj.object_name for obj in client.list_objects(bucket, directory_object_name+\"diccionario_de_datos/\")]    \n",
    "    try:\n",
    "        response = client.get_object(bucket, dic_location[0])\n",
    "        s = str(response.read(),'latin-1')\n",
    "    finally:\n",
    "        response.close()\n",
    "        response.release_conn()\n",
    "    df = pd.read_csv(StringIO(s),names=[\"id\",\"Variable\",\"Descripcion\",\"Tipo\",\"valor\",\"etiqueta_rango\"], index_col=False)\n",
    "    valid_entries = ~pd.to_numeric(df[\"id\"],errors='coerce',downcast='integer').isna()\n",
    "    return df[valid_entries]\n",
    "\n",
    "# Convierte el tipo de columna del estilo datos abiertos al estilo SQL\n",
    "def get_type(entrada):\n",
    "    # TODO: Investigar los tipos de HIve y ver cual se justa mejor\n",
    "    # TODO: agregar lognitudes \n",
    "    lookup = {\"C\":\"STRING\",\"N\":\"DECIMAL\"}\n",
    "    for key in lookup:\n",
    "        if entrada.startswith(key):\n",
    "            return lookup[key]\n",
    "    raise Exception('Variable del tipo \"{}\" no encontrado'.format(entrada))\n",
    "\n",
    "# atraviesa todos los sub-directorios para crear un a tabla para cada uno\n",
    "def create_dataset_tables(client,bucket,data_set):\n",
    "    definitions = [create_hive_table(client, bucket, obj.object_name) for obj in client.list_objects(bucket, data_set) if obj.is_dir]\n",
    "    return \"\\n\".join(definitions)\n",
    "\n",
    "# ToDo: ejecutar directamente en el Hive-Metastore y revisar si la definición ha sido exitosa \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de ejecucion\n",
    "\n",
    "En este ejemplo se toman los datos cargados en el cuaderno CargaObjetos.ipynb para crear las definiciones de tablas.\n",
    "\n",
    "Es necesario copiar la salida de las siguientes celdas y pegarla en el archivo 'services\\hivemetastore\\create-table.hql'\n",
    "y ejecutar el comando \n",
    "```\n",
    "docker-compose exec hive-metastore beeline -u jdbc:hive2:// -f /tmp/create-table.hql\n",
    "```\n",
    "con esto se definiran los datos de las tablas. \n",
    "\n",
    "\n",
    "**Nota:** En los datos a descargar hay un problema con el archivo *sopresa* en la linea *desconocido* es necesario poner comillas para que el csv se detecte de manera correcta. \n",
    "Esto se debe hacer antes de cargarlo a MinIO, si apenas encontraste el error solo es necesario modificarlo y sobreescribir el archivo en MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_dataset_tables(client,\"hive\",\"warehouse/conjunto_de_datos_enigh_2018_ns_csv/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_dataset_tables(client,\"hive\",\"warehouse/conjunto_de_datos_enigh2016_nueva_serie_csv/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_dataset_tables(client,\"hive\",\"warehouse/enigh_ncv_2014_csv/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
