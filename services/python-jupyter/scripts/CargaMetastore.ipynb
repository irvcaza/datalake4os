{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuaderno para cargar metadatos \n",
    "\n",
    "Este cuaderno toma un directorio de MinIO con estructura de datos abiertos y crea la definición para Hive-Metastore para cada una de las tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "import json\n",
    "from pyhive import hive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion de coneccion a MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Minio(\n",
    "        \"minio:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruta al diccionario de correpondencia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_file = None\n",
    "corr_file = 'correspondence.json'\n",
    "#corr_file = 'correspondence.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de ayuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtine el diccionario de correspondencia de un archivo \n",
    "def get_corrr_dic(corr_file = None):\n",
    "    corr_dic = {\"tables\":{},\"columns\":{}}\n",
    "    if corr_file is None:\n",
    "        # Si se pasa None a la funcion dara un diccionario en blanco \n",
    "        pass\n",
    "    elif corr_file.endswith(\".json\"):\n",
    "        with open(corr_file, 'r') as myfile:\n",
    "            corr_dic = json.loads(myfile.read())\n",
    "    elif corr_file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(corr_file)\n",
    "        df.apply(fill_dic,1,new_dic=corr_dic) \n",
    "    else:\n",
    "        raise Exception(\"Format not found, only .json and .csv\")\n",
    "    return corr_dic\n",
    "    \n",
    "# llena los datos en un diccionario de correpondencia con la informacion de un renglón \n",
    "def fill_dic(row,new_dic):\n",
    "    if row[\"type\"] == \"tables\":\n",
    "        new_dic[\"tables\"][row[\"original_name\"]]=row[\"final_name\"]\n",
    "    if row[\"type\"] == \"columns\":\n",
    "        try:\n",
    "            new_dic[\"columns\"][row[\"table\"]][row[\"original_name\"]]=row[\"final_name\"]\n",
    "        except KeyError:\n",
    "            new_dic[\"columns\"][row[\"table\"]] = {}\n",
    "            new_dic[\"columns\"][row[\"table\"]][row[\"original_name\"]]=row[\"final_name\"]\n",
    "\n",
    "# Crea la definicion de la tabla en el directorio dado utilizando el diccionario de datos\n",
    "def create_hive_table(client, bucket, directory_object_name):\n",
    "    table_name = get_table_name(directory_object_name)\n",
    "    col_def = get_col_def(client, bucket, directory_object_name)\n",
    "    data_location = directory_object_name+\"conjunto_de_datos/\"\n",
    "    # TODO: revisar si la tabla ya existe \n",
    "    table_def = \"\"\" CREATE EXTERNAL TABLE {} ({})\n",
    "        ROW FORMAT DELIMITED\n",
    "        FIELDS TERMINATED BY ','\n",
    "        LINES TERMINATED BY '\\\\n'\n",
    "        LOCATION 's3a://{}/{}'\n",
    "        TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "        \"\"\".format(table_name,col_def,bucket,data_location)\n",
    "    return table_def\n",
    "\n",
    "\n",
    "# Funcion para establecer el nombre de la tabla, en una version peeliminar es el nombre del directorio, pero se puede refinar para lograr algo mas conciso \n",
    "def get_table_name(directory_object_name):\n",
    "    old_name = directory_object_name.split(\"/\")[-2]\n",
    "    try:\n",
    "        new_name = corr_dic[\"tables\"][old_name]\n",
    "    except KeyError:\n",
    "        new_name = old_name\n",
    "    return new_name\n",
    "\n",
    "def get_col_name(variable,table_name):\n",
    "    try:\n",
    "        new_name = corr_dic[\"columns\"][table_name][variable]\n",
    "    except KeyError:\n",
    "        new_name = variable\n",
    "    return new_name\n",
    "    \n",
    "# Fucion que crea la definicion de las variables, nombre de la variable y tipo \n",
    "def get_col_def (client, bucket, directory_object_name):\n",
    "    data_dictionary = get_data_dictionary(client, bucket, directory_object_name)\n",
    "    # TODO: revisar si este ese el orden real de las columans de la tabla \n",
    "    table_name = get_table_name(directory_object_name)\n",
    "    names = [get_col_name(x,table_name) for x in data_dictionary[\"Variable\"]]\n",
    "    types = [get_type(x) for x in data_dictionary[\"Tipo\"]]\n",
    "    return \", \".join([\"{} {}\".format(x,y) for x,y in zip(names,types)])\n",
    "    \n",
    "# Obtiene el diccionario de datos del formato de datos abiertos para definir las varibles\n",
    "def get_data_dictionary(client, bucket, directory_object_name):\n",
    "    dic_location = [obj.object_name for obj in client.list_objects(bucket, directory_object_name+\"diccionario_de_datos/\")]    \n",
    "    try:\n",
    "        response = client.get_object(bucket, dic_location[0])\n",
    "        s = str(response.read(),'latin-1')\n",
    "    finally:\n",
    "        response.close()\n",
    "        response.release_conn()\n",
    "    df = pd.read_csv(StringIO(s),names=[\"id\",\"Variable\",\"Descripcion\",\"Tipo\",\"valor\",\"etiqueta_rango\"], index_col=False)\n",
    "    valid_entries = ~pd.to_numeric(df[\"id\"],errors='coerce',downcast='integer').isna()\n",
    "    return df[valid_entries]\n",
    "\n",
    "# Convierte el tipo de columna del estilo datos abiertos al estilo SQL\n",
    "def get_type(entrada):\n",
    "    # TODO: Investigar los tipos de HIve y ver cual se justa mejor\n",
    "    # TODO: agregar lognitudes \n",
    "    lookup = {\"C\":\"STRING\",\"N\":\"DECIMAL\"}\n",
    "    for key in lookup:\n",
    "        if entrada.startswith(key):\n",
    "            return lookup[key]\n",
    "    raise Exception('Variable del tipo \"{}\" no encontrado'.format(entrada))\n",
    "\n",
    "# atraviesa todos los sub-directorios para crear un a tabla para cada uno\n",
    "def create_dataset_tables(client,bucket,data_set):\n",
    "    definitions = [create_hive_table(client, bucket, obj.object_name) for obj in client.list_objects(bucket, data_set) if obj.is_dir]\n",
    "    return definitions\n",
    "\n",
    "# ToDo: ejecutar directamente en el Hive-Metastore y revisar si la definición ha sido exitosa \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de ejecucion\n",
    "\n",
    "En este ejemplo se toman los datos cargados en el cuaderno CargaObjetos.ipynb para crear las definiciones de tablas.\n",
    "\n",
    "Es necesario copiar la salida de las siguientes celdas y pegarla en el archivo 'services\\hivemetastore\\create-table.hql'\n",
    "y ejecutar el comando \n",
    "```\n",
    "docker-compose exec hive-metastore beeline -u jdbc:hive2:// -f /tmp/create-table.hql\n",
    "```\n",
    "con esto se definiran los datos de las tablas. \n",
    "\n",
    "\n",
    "**Nota:** En los datos a descargar hay un problema con el archivo *conjunto_de_datos_enigh_2018_ns_csv\\conjunto_de_datos_poblacion_enigh_2018_ns\\diccionario_de_datos\\diccionario_datos_poblacion_enigh_2018_ns.csv* en la linea *81* es necesario poner comillas para que el csv se detecte de manera correcta. \n",
    "Se puede hacer de manera automatica en el cuaderno DescargaDatos.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dic = get_corrr_dic(corr_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqls= create_dataset_tables(client,\"hive\",\"warehouse/conjunto_de_datos_enigh_2018_ns_csv/\")\n",
    "\n",
    "#print(create_dataset_tables(client,\"hive\",\"warehouse/conjunto_de_datos_enigh2016_nueva_serie_csv/\"))\n",
    "\n",
    "#print(create_dataset_tables(client,\"hive\",\"warehouse/enigh_ncv_2014_csv/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection(host='hive-server',port=10000,auth='NOSASL',database=\"default\"):\n",
    "    conn = hive.Connection(host=host,port=port,auth=auth,database=database)\n",
    "    return conn\n",
    "\n",
    "def procc_SQL_list(list_sqls):\n",
    "    conn = get_connection()\n",
    "    cursor = conn.cursor()\n",
    "    for sql in list_sqls:\n",
    "        try:\n",
    "            cursor.execute(sql)\n",
    "        except:\n",
    "            print(\" Error al crear la tabla \"+sql.split(\" \")[11])\n",
    "    conn.close()\n",
    "\n",
    "def drop_list_tables(list_sqls):\n",
    "    conn = get_connection()\n",
    "    cursor = conn.cursor()\n",
    "    for sql in list_sqls:\n",
    "        try:\n",
    "            cursor.execute(\"DROP TABLE \"+sql.split(\" \")[3])\n",
    "        except:\n",
    "            print(\"Error al eliminar tabla \"+sql.split(\" \")[3])\n",
    "    conn.close()\n",
    "\n",
    "    \n",
    "def list_tables():\n",
    "    conn = get_connection()\n",
    "    try:\n",
    "        df = pd.read_sql(\"SHOW TABLES\", conn)\n",
    "    except:\n",
    "        print(\"Error al listar tablas\")\n",
    "    conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procc_SQL_list(sqls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
